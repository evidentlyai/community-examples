{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and imports"
      ],
      "metadata": {
        "id": "Tdvm881pyC8w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thDJOro8nijJ"
      },
      "outputs": [],
      "source": [
        "!pip install evidently[llm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIVmb7pBxyHp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from evidently import Dataset\n",
        "from evidently import DataDefinition\n",
        "from evidently import Report\n",
        "from evidently import BinaryClassification\n",
        "from evidently.descriptors import *\n",
        "from evidently.presets import TextEvals, ValueStats, ClassificationPreset, DataSummaryPreset\n",
        "from evidently.metrics import *\n",
        "\n",
        "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
        "\n",
        "from evidently.sdk.models import PanelMetric\n",
        "from evidently.sdk.panels import DashboardPanelPlot\n",
        "\n",
        "from evidently.ui.workspace import CloudWorkspace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we will:\n",
        "- Define the evaluation criteria for our LLM judge\n",
        "- Build an LLM-as-a-Judge using different prompts/models\n",
        "- Evaluate the quality of the judge comparing results to human labels"
      ],
      "metadata": {
        "id": "evPnRvlIdX8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Set up Evidently Cloud"
      ],
      "metadata": {
        "id": "8Tzr4GcygLNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up API keys for LLM judges:"
      ],
      "metadata": {
        "id": "8ZbeRCC-9tKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import os\n",
        "## os.environ[\"OPENAI_API_KEY\"] = \"OPEN_AI_API_KEY\"\n",
        "## os.environ[\"ANTHROPIC_API_KEY\"] = \"ANTHROPIC_API_KEY\""
      ],
      "metadata": {
        "id": "h5jVf-0TJQ6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional**. Connect to Cloud and create a Project:"
      ],
      "metadata": {
        "id": "3WdDmKNmghNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ws = CloudWorkspace(token=\"YOUR_API_TOKEN\", url=\"https://app.evidently.cloud\")"
      ],
      "metadata": {
        "id": "TfNSnYQTgK4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#project = ws.create_project(\"My project name\", org_id=\"YOUR_ORG_ID\")\n",
        "#project.description = \"My project description\"\n",
        "\n",
        "# or project = ws.get_project(\"PROJECT_ID\")"
      ],
      "metadata": {
        "id": "qv9EblcaxDpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNFjnBiN382e"
      },
      "source": [
        "# Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noZ_CNMd4CCY"
      },
      "source": [
        "We start with an expert-labeled dataset. We will use it as the ground truth for our LLM judge."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/evidentlyai/community-examples/main/datasets/code_review_dataset.csv\"\n",
        "review_dataset = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "uVu_RLf026mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview:"
      ],
      "metadata": {
        "id": "HlEq15vC3EpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NnMJHK6y0r2"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "review_dataset.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an Evidently dataset:"
      ],
      "metadata": {
        "id": "zyQBJx0P-YtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "definition = DataDefinition(\n",
        "    text_columns=[\"Generated review\", \"Expert comment\"],\n",
        "    categorical_columns=[\"Expert label\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "TxS43-M7GOEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = Dataset.from_pandas(\n",
        "    pd.DataFrame(review_dataset),\n",
        "    data_definition=definition)"
      ],
      "metadata": {
        "id": "jmp8HGvvGEen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preview the distribution of classes:"
      ],
      "metadata": {
        "id": "jYTDRwu9I3PG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([\n",
        "  ValueStats(column=\"Expert label\")\n",
        "])\n",
        "\n",
        "my_eval = report.run(eval_dataset)\n",
        "my_eval"
      ],
      "metadata": {
        "id": "yUZkYba1Gv7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional**. Let's upload the source dataset to Evidently Cloud."
      ],
      "metadata": {
        "id": "HveSNA6K07PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws.add_dataset(\n",
        "    dataset = eval_dataset,\n",
        "    name = \"source_dataset\",\n",
        "    project_id = project.id,\n",
        "    description = \"Dataset with expert labels on review quality\")"
      ],
      "metadata": {
        "id": "jEg5Mlgh1Cgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our goal: create LLM judge to match the human labels"
      ],
      "metadata": {
        "id": "PUZ8DoH609KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Options**:\n",
        "- Splitting criteria: (actionable / non-actionable, appropriate tone / inappropriate tone).\n",
        "- Try create a good/bad judge. (It may be useful to introduce a borderline or \"needs review\" tag for subtle or new cases)."
      ],
      "metadata": {
        "id": "kIpXFb9MMtwb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uifPvCmI34sj"
      },
      "source": [
        "# Exp 1. Design the LLM judge - First try"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the tutorial flow, we'll keep the steps explicit and run 5 sequential experiments."
      ],
      "metadata": {
        "id": "3937d3EseMUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First attempt to create the judge:"
      ],
      "metadata": {
        "id": "j3zPanZttrRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Name the experiment\n",
        "name = \"naive_prompt\"\n",
        "\n",
        "# 2. Define LLM judge prompt template\n",
        "feedback_quality = BinaryClassificationPromptTemplate(\n",
        "        pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
        "        criteria = \"\"\"An review is GOOD when it's actionable and constructive.\n",
        "        A review is BAD when is non-actionable or overly critical.\n",
        "        \"\"\",\n",
        "        target_category=\"bad\",\n",
        "        non_target_category=\"good\",\n",
        "        uncertainty=\"unknown\",\n",
        "        include_reasoning=True,\n",
        "        )\n",
        "\n",
        "# 3. Apply the LLM judge\n",
        "eval_dataset = Dataset.from_pandas(\n",
        "    pd.DataFrame(review_dataset),\n",
        "    data_definition=definition,\n",
        "    descriptors=[\n",
        "        LLMEval(\"Generated review\",\n",
        "                template=feedback_quality,  # We can pass new prompt version\n",
        "                provider=\"openai\",          # We can change the provider\n",
        "                model=\"gpt-4o-mini\",        # We can change the model\n",
        "                alias=\"LLM-judged quality\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Add TRUE/FALSE for judge alignment\n",
        "eval_dataset.add_descriptors([\n",
        "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
        "])"
      ],
      "metadata": {
        "id": "BMnM3wvjiHUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(feedback_quality.get_template())"
      ],
      "metadata": {
        "id": "cJEEClFvBW4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM judgments**. View all results locally:"
      ],
      "metadata": {
        "id": "FBIT1vvti8jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset.as_dataframe()"
      ],
      "metadata": {
        "id": "lsa6mxqTi-8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**. Let's summarize:"
      ],
      "metadata": {
        "id": "FwQxc05EjUjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report = Report([\n",
        "    TextEvals()\n",
        "])\n",
        "\n",
        "my_eval = report.run(eval_dataset)\n",
        "my_eval"
      ],
      "metadata": {
        "id": "ecGF6dv4jrV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification quality**. This function runs the Classification Report to evaluate the LLM judge quality and optionally uploads it to Evidently Cloud (if the workspace is set) with a tag.\n"
      ],
      "metadata": {
        "id": "9P5xYTeJj-7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_classification_report(eval_dataset, name=None, cloud_ws=None, project_id=None):\n",
        "\n",
        "    df = eval_dataset.as_dataframe()\n",
        "    df_filtered = df[df[\"LLM-judged quality\"] != \"UNKNOWN\"]\n",
        "\n",
        "    # Set the classification Data Definition\n",
        "    definition_class = DataDefinition(\n",
        "        classification=[BinaryClassification(\n",
        "            target=\"Expert label\",\n",
        "            prediction_labels=\"LLM-judged quality\",\n",
        "            pos_label=\"bad\"\n",
        "        )],\n",
        "        categorical_columns=[\"Expert label\", \"LLM-judged quality\"]\n",
        "    )\n",
        "\n",
        "    # Create a Dataset object\n",
        "    eval_data = Dataset.from_pandas(df_filtered, data_definition=definition_class)\n",
        "\n",
        "    # Build classification report\n",
        "    report = Report([\n",
        "        ClassificationPreset(),\n",
        "        ValueStats(\"LLM-judged quality\"),\n",
        "        ValueStats(\"Expert label\")\n",
        "    ])\n",
        "\n",
        "    # Apply tag(s)\n",
        "    tags = [name] if name else []\n",
        "\n",
        "    my_eval = report.run(eval_data, tags=tags)\n",
        "\n",
        "    # Optional: upload to Evidently Cloud\n",
        "    if cloud_ws and project_id:\n",
        "        cloud_ws.add_run(project_id, my_eval, include_data=True)\n",
        "\n",
        "    return my_eval"
      ],
      "metadata": {
        "id": "hn5fpk42nNAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(See all Evidently Metrics and Presets: https://docs.evidentlyai.com/metrics/all_metrics)"
      ],
      "metadata": {
        "id": "wRnIRDZhpefV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the function to evaluate the LLM judge quality:"
      ],
      "metadata": {
        "id": "q6cNjqvxoCkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_eval = run_classification_report(\n",
        "    eval_dataset,\n",
        "    name=name,\n",
        "    cloud_ws=ws, #Optional\n",
        "    project_id=project.id #Optional\n",
        ")"
      ],
      "metadata": {
        "id": "Ta9MOaHYpr2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also preview the classification report locally:"
      ],
      "metadata": {
        "id": "JKPozp0wD4Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_eval"
      ],
      "metadata": {
        "id": "AFUMhtNlqeog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Optional) Add dashboard-as-code"
      ],
      "metadata": {
        "id": "eRl14wXsq5Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a dashboard to track evaluation results over time."
      ],
      "metadata": {
        "id": "hywgxAwf0lwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project.dashboard.add_panel(\n",
        "             DashboardPanelPlot(\n",
        "                title=\"LLM judge quality\",\n",
        "                subtitle = \"Quality of the LLM judge that evaluates reviews compared to human labels.\",\n",
        "                size=\"full\",\n",
        "                values=[\n",
        "                    PanelMetric(\n",
        "                        metric=\"Precision\",\n",
        "                        legend=\"Precision\"\n",
        "                    ),\n",
        "                    PanelMetric(\n",
        "                        metric=\"Recall\",\n",
        "                        legend=\"Recall\"\n",
        "                    ),\n",
        "                    PanelMetric(\n",
        "                        metric=\"Accuracy\",\n",
        "                        legend=\"Accuracy\"\n",
        "                    ),\n",
        "                ],\n",
        "                plot_params={\"plot_type\": \"line\"},\n",
        "            ),\n",
        "            tab=\"LLM evals\",\n",
        "        )"
      ],
      "metadata": {
        "id": "OwhlTUfDzdob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 2. Try another prompt"
      ],
      "metadata": {
        "id": "_tzWBsYmNQ2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try writing a more detailed prompt:"
      ],
      "metadata": {
        "id": "7d2yACaDuRJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Name the experiment <- new name\n",
        "name = \"detailed_prompt\"\n",
        "\n",
        "# 2. Define LLM judge prompt template  <- new prompt\n",
        "feedback_quality_2 = BinaryClassificationPromptTemplate(\n",
        "    pre_messages=[(\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
        "    criteria=\"\"\"\n",
        "    A review is **GOOD** if it is actionable and constructive. It should:\n",
        "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
        "    - Be respectful and encourage learning or improvement\n",
        "    - Use professional, helpful language—even when pointing out problems\n",
        "\n",
        "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
        "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
        "    - It may focus on praise only, without offering guidance\n",
        "    - It may sound dismissive, contradictory, harsh, or robotic\n",
        "    - It may raise a concern but fail to explain what should be done\n",
        "    \"\"\",\n",
        "    target_category=\"bad\",\n",
        "    non_target_category=\"good\",\n",
        "    uncertainty=\"unknown\",\n",
        "    include_reasoning=True,\n",
        ")\n",
        "\n",
        "# 3. Apply the LLM judge\n",
        "eval_dataset = Dataset.from_pandas(\n",
        "    pd.DataFrame(review_dataset),\n",
        "    data_definition=definition,\n",
        "    descriptors=[\n",
        "        LLMEval(\"Generated review\",\n",
        "                template=feedback_quality_2,# We can pass new prompt version <- new prompt\n",
        "                provider=\"openai\",          # We can change the provider\n",
        "                model=\"gpt-4o-mini\",        # We can change the model\n",
        "                alias=\"LLM-judged quality\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Add TRUE/FALSE for judge alignment\n",
        "eval_dataset.add_descriptors([\n",
        "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
        "])"
      ],
      "metadata": {
        "id": "1mRVei9isytp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the LLM judge quality:"
      ],
      "metadata": {
        "id": "l-mahKjhusIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_eval = run_classification_report(\n",
        "    eval_dataset,\n",
        "    name=name,\n",
        "    cloud_ws=ws, #Optional\n",
        "    project_id=project.id #Optional\n",
        ")"
      ],
      "metadata": {
        "id": "NMflTRZWuurK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 3. Can we make it better?"
      ],
      "metadata": {
        "id": "M19OtYbSVxOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Name the experiment <- new name\n",
        "name = \"detailed_prompt_think_better\"\n",
        "\n",
        "# 2. Define LLM judge prompt template  <- new prompt\n",
        "feedback_quality_3 = BinaryClassificationPromptTemplate(\n",
        "    pre_messages=[\n",
        "        (\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
        "    criteria=\"\"\"\n",
        "    A review is **GOOD** if it is actionable and constructive. It should:\n",
        "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
        "    - Be respectful and encourage learning or improvement\n",
        "    - Use professional, helpful language—even when pointing out problems\n",
        "\n",
        "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
        "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
        "    - It may focus on praise only, without offering guidance\n",
        "    - It may sound dismissive, contradictory, harsh, or robotic\n",
        "    - It may raise a concern but fail to explain what should be done\n",
        "\n",
        "    Always explain your reasoning.\n",
        "    \"\"\",\n",
        "    target_category=\"bad\",\n",
        "    non_target_category=\"good\",\n",
        "    uncertainty=\"unknown\",\n",
        "    include_reasoning=True,\n",
        ")\n",
        "\n",
        "# 3. Apply the LLM judge\n",
        "eval_dataset = Dataset.from_pandas(\n",
        "    pd.DataFrame(review_dataset),\n",
        "    data_definition=definition,\n",
        "    descriptors=[\n",
        "        LLMEval(\"Generated review\",\n",
        "                template=feedback_quality_3,# We can pass new prompt version  <- new prompt\n",
        "                provider=\"openai\",          # We can change the provider\n",
        "                model=\"gpt-4o-mini\",        # We can change the model\n",
        "                alias=\"LLM-judged quality\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Add TRUE/FALSE for judge alignment\n",
        "eval_dataset.add_descriptors([\n",
        "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
        "])"
      ],
      "metadata": {
        "id": "Rb9MkrcpwRhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_eval = run_classification_report(\n",
        "    eval_dataset,\n",
        "    name=name,\n",
        "    cloud_ws=ws, #Optional\n",
        "    project_id=project.id #Optional\n",
        ")"
      ],
      "metadata": {
        "id": "JBCDLhU_witm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#my_eval"
      ],
      "metadata": {
        "id": "qnGApwNbwtiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 4. Try a different model (Turbo)"
      ],
      "metadata": {
        "id": "xf3OsJHQV5-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can a cheaper, simpler model perform as well?"
      ],
      "metadata": {
        "id": "cfuJY18KxlH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Name the experiment  <- new name\n",
        "name = \"turbo\"\n",
        "\n",
        "# 2. Define LLM judge prompt template\n",
        "feedback_quality_3 = BinaryClassificationPromptTemplate(\n",
        "    pre_messages=[\n",
        "        (\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
        "    criteria=\"\"\"\n",
        "    A review is **GOOD** if it is actionable and constructive. It should:\n",
        "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
        "    - Be respectful and encourage learning or improvement\n",
        "    - Use professional, helpful language—even when pointing out problems\n",
        "\n",
        "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
        "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
        "    - It may focus on praise only, without offering guidance\n",
        "    - It may sound dismissive, contradictory, harsh, or robotic\n",
        "    - It may raise a concern but fail to explain what should be done\n",
        "\n",
        "    Always explain your reasoning.\n",
        "    \"\"\",\n",
        "    target_category=\"bad\",\n",
        "    non_target_category=\"good\",\n",
        "    uncertainty=\"unknown\",\n",
        "    include_reasoning=True,\n",
        ")\n",
        "\n",
        "# 3. Apply the LLM judge\n",
        "eval_dataset = Dataset.from_pandas(\n",
        "    pd.DataFrame(review_dataset),\n",
        "    data_definition=definition,\n",
        "    descriptors=[\n",
        "        LLMEval(\"Generated review\",\n",
        "                template=feedback_quality_3,  # We can pass new prompt version\n",
        "                provider=\"openai\",            # We can change the provider\n",
        "                model=\"gpt-3.5-turbo\",        # We can change the model  <- different model\n",
        "                alias=\"LLM-judged quality\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Add TRUE/FALSE for judge alignment\n",
        "eval_dataset.add_descriptors([\n",
        "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
        "])"
      ],
      "metadata": {
        "id": "2EEEvf7MxjPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_eval = run_classification_report(\n",
        "    eval_dataset,\n",
        "    name=name,\n",
        "    cloud_ws=ws, #Optional\n",
        "    project_id=project.id #Optional\n",
        ")"
      ],
      "metadata": {
        "id": "YZI3hV2MyV1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exp 5. Try another provider (Anthropic)"
      ],
      "metadata": {
        "id": "RwEcCT4xWOgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evidently.legacy.utils.llm.wrapper import AnthropicOptions"
      ],
      "metadata": {
        "id": "8OAM3KLJW8B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Name the experiment <- new name\n",
        "name = \"anthropic\"\n",
        "\n",
        "# 2. Define LLM judge prompt template\n",
        "feedback_quality_3 = BinaryClassificationPromptTemplate(\n",
        "    pre_messages=[\n",
        "        (\"system\", \"You are evaluating the quality of code reviews given to junior developers.\")],\n",
        "    criteria=\"\"\"\n",
        "    A review is **GOOD** if it is actionable and constructive. It should:\n",
        "    - Offer clear, specific suggestions or highlight issues in a way that the developer can address\n",
        "    - Be respectful and encourage learning or improvement\n",
        "    - Use professional, helpful language—even when pointing out problems\n",
        "\n",
        "    A review is **BAD** if it is non-actionable or overly critical. For example:\n",
        "    - It may be vague, generic, or hedged to the point of being unhelpful\n",
        "    - It may focus on praise only, without offering guidance\n",
        "    - It may sound dismissive, contradictory, harsh, or robotic\n",
        "    - It may raise a concern but fail to explain what should be done\n",
        "\n",
        "    Always explain your reasoning.\n",
        "    \"\"\",\n",
        "    target_category=\"bad\",\n",
        "    non_target_category=\"good\",\n",
        "    uncertainty=\"unknown\",\n",
        "    include_reasoning=True,\n",
        ")\n",
        "\n",
        "# 3. Apply the LLM judge\n",
        "eval_dataset = Dataset.from_pandas(\n",
        "    pd.DataFrame(review_dataset),\n",
        "    data_definition=definition,\n",
        "    descriptors=[\n",
        "        LLMEval(\"Generated review\",\n",
        "                template=feedback_quality_3,         # We can pass new prompt version\n",
        "                provider=\"anthropic\",                # We can change the provider <- new provider\n",
        "                model=\"claude-3-5-sonnet-20240620\",  # We can change the model <- new model\n",
        "                alias=\"LLM-judged quality\")\n",
        "    ],\n",
        "    options=AnthropicOptions(rpm_limit=50)  # <- rate limit params\n",
        ")\n",
        "\n",
        "# 4. Add TRUE/FALSE for judge alignment\n",
        "eval_dataset.add_descriptors([\n",
        "    ExactMatch(columns=[\"LLM-judged quality\", \"Expert label\"], alias=\"Judge_alignment\")\n",
        "])"
      ],
      "metadata": {
        "id": "_UkMiAAzy5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_eval = run_classification_report(\n",
        "    eval_dataset,\n",
        "    name=name,\n",
        "    cloud_ws=ws, #Optional\n",
        "    project_id=project.id #Optional\n",
        ")"
      ],
      "metadata": {
        "id": "evKn7rAOzQsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What's next?"
      ],
      "metadata": {
        "id": "85f045lnC_xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you make a better prompt? Conside splitting the criteria - and review the labels first! (Even humans don't always agree :)"
      ],
      "metadata": {
        "id": "w3r1rbowDBl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enjoyed the tutorial? Star Evidently on GitHub to support the project: https://github.com/evidentlyai/evidently"
      ],
      "metadata": {
        "id": "lx-s7J72DSF1"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}